{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2114594f-8a3e-4973-82db-89b8f289a0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hexarena.env import ForagingEnv\n",
    "\n",
    "env = ForagingEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16a105b0-3fa8-47d9-81dd-2df582d85720",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, _ = env.reset()\n",
    "\n",
    "obss = []\n",
    "for _ in range(50):\n",
    "    observation, *_ = env.step(env.action_space.sample())\n",
    "    obss.append([*observation['monkey'], *observation['color'], observation['rewarded']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebf032d9-fdb8-4f71-8e45-988d5cfdcd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('color': Box(-inf, inf, (2,), float32), 'monkey': MultiDiscrete([19 19]), 'rewarded': Discrete(2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13b72a7b-7367-4400-9cf8-3739f41cec2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('cue': Box(0.0, 1.0, (1,), float32), 'food': Discrete(2))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state_space['box_0']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3587e001-684c-4d74-9255-fb2a8add0aec",
   "metadata": {},
   "source": [
    "from stable_baselines3 import PPO\n",
    "from irc.utils import ProgressBarCallback\n",
    "from jarvis.utils import tqdm\n",
    "\n",
    "num_steps = 2**5\n",
    "algo = PPO(env=env, policy='MultiInputPolicy')\n",
    "algo.policy.set_training_mode(True)\n",
    "# with tqdm(total=num_steps, unit='step', desc='Training agent') as pbar:\n",
    "#     algo.learn(\n",
    "#         total_timesteps=num_steps,\n",
    "#         # callback=ProgressBarCallback(pbar),\n",
    "#     )\n",
    "algo.learn(total_timesteps=num_steps, progress_bar=True)\n",
    "algo.policy.set_training_mode(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1951df8a-3f08-4f56-8831-6609f68bb32c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Policy MultiInputPolicy unknown",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m n_steps = \u001b[32m200\u001b[39m \u001b[38;5;66;03m# time steps per update in PPO\u001b[39;00m\n\u001b[32m      7\u001b[39m batch_size = \u001b[32m100\u001b[39m \u001b[38;5;66;03m# batch size in PPO\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m algo = \u001b[43mRecurrentPPO\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mMultiInputPolicy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m n_updates = \u001b[32m100\u001b[39m\n\u001b[32m     15\u001b[39m algo.policy.set_training_mode(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cp312/lib/python3.12/site-packages/sb3_contrib/ppo_recurrent/ppo_recurrent.py:101\u001b[39m, in \u001b[36mRecurrentPPO.__init__\u001b[39m\u001b[34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, target_kl, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     75\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     76\u001b[39m     policy: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtype\u001b[39m[RecurrentActorCriticPolicy]],\n\u001b[32m   (...)\u001b[39m\u001b[32m     99\u001b[39m     _init_setup_model: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    100\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgae_lambda\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgae_lambda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43ment_coef\u001b[49m\u001b[43m=\u001b[49m\u001b[43ment_coef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvf_coef\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvf_coef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_sde\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_sde\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m        \u001b[49m\u001b[43msde_sample_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[43msde_sample_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstats_window_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstats_window_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_init_setup_model\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m        \u001b[49m\u001b[43msupported_action_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m            \u001b[49m\u001b[43mspaces\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m            \u001b[49m\u001b[43mspaces\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDiscrete\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m            \u001b[49m\u001b[43mspaces\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMultiDiscrete\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m            \u001b[49m\u001b[43mspaces\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMultiBinary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28mself\u001b[39m.batch_size = batch_size\n\u001b[32m    129\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_epochs = n_epochs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cp312/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:86\u001b[39m, in \u001b[36mOnPolicyAlgorithm.__init__\u001b[39m\u001b[34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, rollout_buffer_class, rollout_buffer_kwargs, stats_window_size, tensorboard_log, monitor_wrapper, policy_kwargs, verbose, seed, device, _init_setup_model, supported_action_spaces)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     62\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     63\u001b[39m     policy: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtype\u001b[39m[ActorCriticPolicy]],\n\u001b[32m   (...)\u001b[39m\u001b[32m     84\u001b[39m     supported_action_spaces: Optional[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mtype\u001b[39m[spaces.Space], ...]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     85\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_sde\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_sde\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43msde_sample_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[43msde_sample_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43msupport_multi_env\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmonitor_wrapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmonitor_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstats_window_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstats_window_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m        \u001b[49m\u001b[43msupported_action_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43msupported_action_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_steps = n_steps\n\u001b[32m    104\u001b[39m     \u001b[38;5;28mself\u001b[39m.gamma = gamma\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cp312/lib/python3.12/site-packages/stable_baselines3/common/base_class.py:124\u001b[39m, in \u001b[36mBaseAlgorithm.__init__\u001b[39m\u001b[34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    107\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    108\u001b[39m     policy: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtype\u001b[39m[BasePolicy]],\n\u001b[32m   (...)\u001b[39m\u001b[32m    121\u001b[39m     supported_action_spaces: Optional[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mtype\u001b[39m[spaces.Space], ...]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    122\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(policy, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28mself\u001b[39m.policy_class = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_policy_from_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    126\u001b[39m         \u001b[38;5;28mself\u001b[39m.policy_class = policy\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cp312/lib/python3.12/site-packages/stable_baselines3/common/base_class.py:339\u001b[39m, in \u001b[36mBaseAlgorithm._get_policy_from_name\u001b[39m\u001b[34m(self, policy_name)\u001b[39m\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.policy_aliases[policy_name]\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPolicy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpolicy_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m unknown\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Policy MultiInputPolicy unknown"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from jarvis.utils import tqdm\n",
    "from irc.utils import ProgressBarCallback\n",
    "\n",
    "n_steps = 200 # time steps per update in PPO\n",
    "batch_size = 100 # batch size in PPO\n",
    "\n",
    "algo = RecurrentPPO(\n",
    "    policy='MultiInputPolicy', env=env,\n",
    "    n_steps=n_steps, batch_size=batch_size,\n",
    ")\n",
    "\n",
    "n_updates = 100\n",
    "algo.policy.set_training_mode(True)\n",
    "with tqdm(total=n_updates, unit='update', leave=False) as pbar:\n",
    "    pbar_cb = ProgressBarCallback(pbar, disp_freq=n_steps)\n",
    "    # pbar_cb.logs = manager.logs\n",
    "    # if len(manager.logs)>0:\n",
    "    #     pbar_cb.reward, pbar_cb.food = manager.logs[-1]\n",
    "    algo.env.envs[0].needs_reset = False\n",
    "    algo.learn(\n",
    "        total_timesteps=n_updates*n_steps,\n",
    "        callback=pbar_cb, reset_num_timesteps=False,\n",
    "    )\n",
    "algo.policy.set_training_mode(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
